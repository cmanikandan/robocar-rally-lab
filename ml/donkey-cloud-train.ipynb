{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training in the cloud\n",
    "\n",
    "This far, we've looked at the `Donkey` library (the data produced, tools, training and the CNN itself). We saw that the training script is a simple python application, [donkey2.py](https://github.com/wroscoe/donkey/blob/master/donkeycar/templates/donkey2.py), that creates and trains the CNN using the [Keras](https://keras.io) framework. Some of the hyperparameters are defined in a config file, [config_defaults.py](https://github.com/wroscoe/donkey/blob/master/donkeycar/templates/config_defaults.py) and easily customized. The end result of a training run is a trained Keras/TensorFlow model, which can be used to drive the car.\n",
    "\n",
    "But why do the model have to be created and trained using the `Donkey` library and the `Vehicle` abstraction? As long as the end result is the same (a Keras/Tensorflow model), it shouldn't matter where and how it was trained.\n",
    "\n",
    "In this chapter, we'll look into creating and training the CNN using `SageMaker` libraries instead of the Keras/Tensorflow libraries used in `Donkey` (see [Conda dependencies](https://github.com/wroscoe/donkey/blob/master/envs/ubuntu.yml)). Turns out, it is not much different from the `cnn.py` file we created in the last chapter. By moving the training to the cloud, it allows us to more easily focus on improving e.g:\n",
    "- Training performance, e.g. newer framework versions, training acceleration (GPU(s)), training distribution (cluster of training instances), etc.\n",
    "- Training visualization (from a learning perspective) using e.g. TensorBoard\n",
    "- Network tuning\n",
    "- Whatever you can come up with =)\n",
    "\n",
    "## Porting Keras\n",
    "\n",
    "The Keras library has been adopted by TensorFlow in newer version of that library. By porting our code from Keras to the TensorFlow Keras API, we'll have more flexibility and one less dependency. Yay!\n",
    "\n",
    "The first step is to use the code from `cnn.py` in this notebook. SageMaker Jupyter notebooks runs a newer version of the TensorFlow library (see https://docs.aws.amazon.com/sagemaker/latest/dg/supported-versions.html), which presents us with more options, but can also lead to incompability issues with the version runing on the car.\n",
    "\n",
    "FYI, print the TensorFlow version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First, we need to port the original Keras library to the Keras API implemented by newer version of TensorFlow, see:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "The Keras API is imported like:\n",
    "```python\n",
    "import tensorflow.python.keras\n",
    "```\n",
    "\n",
    "To port the code, just add `tensorflow.python.` to any import using `keras`. We'll also clean up some of the dependencies that weren't used, and rename the base class just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "class MyEstimator():\n",
    "    '''\n",
    "    The Estimator creates and trains the model.\n",
    "    \n",
    "    Renamed from MyPilot.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.model = default_categorical()\n",
    "\n",
    "    def train(self, train_gen, val_gen, saved_model_path,\n",
    "              epochs=100, steps=100, train_split=0.8, verbose=1,\n",
    "              min_delta=.0005, patience=5, use_early_stop=True):\n",
    "\n",
    "        save_best = ModelCheckpoint(saved_model_path, \n",
    "                                    monitor='val_loss', \n",
    "                                    verbose=verbose, \n",
    "                                    save_best_only=True, \n",
    "                                    mode='min')\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=min_delta, \n",
    "                                   patience=patience, \n",
    "                                   verbose=verbose, \n",
    "                                   mode='auto')\n",
    "\n",
    "        callbacks_list = [save_best]\n",
    "\n",
    "        if use_early_stop:\n",
    "            callbacks_list.append(early_stop)\n",
    "        \n",
    "        hist = self.model.fit_generator(\n",
    "                        train_gen, \n",
    "                        steps_per_epoch=steps, \n",
    "                        epochs=epochs, \n",
    "                        verbose=1, \n",
    "                        validation_data=val_gen,\n",
    "                        callbacks=callbacks_list, \n",
    "                        validation_steps=steps*(1.0 - train_split))\n",
    "        return hist\n",
    "\n",
    "def default_categorical(): \n",
    "    from tensorflow.python.keras.models import Model\n",
    "    from tensorflow.python.keras.layers import Convolution2D\n",
    "    from tensorflow.python.keras.layers import Input, Dropout, Flatten, Dense\n",
    "    \n",
    "    img_in = Input(shape=(120, 160, 3), name='img_in')\n",
    "    x = img_in\n",
    "\n",
    "    x = Convolution2D(24, (5,5), strides=(2,2), activation='relu')(x)\n",
    "    x = Convolution2D(32, (5,5), strides=(2,2), activation='relu')(x)\n",
    "    x = Convolution2D(64, (5,5), strides=(2,2), activation='relu')(x)\n",
    "    x = Convolution2D(64, (3,3), strides=(2,2), activation='relu')(x)\n",
    "    x = Convolution2D(64, (3,3), strides=(1,1), activation='relu')(x)\n",
    "\n",
    "    x = Flatten(name='flattened')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(.1)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    x = Dropout(.1)(x)\n",
    "\n",
    "    angle_out = Dense(15, activation='softmax', name='angle_out')(x)\n",
    "    throttle_out = Dense(1, activation='relu', name='throttle_out')(x)\n",
    "    \n",
    "    model = Model(inputs=[img_in], outputs=[angle_out, throttle_out])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'angle_out': 'categorical_crossentropy', 'throttle_out': 'mean_absolute_error'},\n",
    "                  loss_weights={'angle_out': 0.9, 'throttle_out': .001})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. This class has no `Donkey` dependencies, which makes it easy to port.\n",
    "\n",
    "## Train the ported model\n",
    "\n",
    "### Install Donkey (again)\n",
    "\n",
    "To train the new model, we still need to read car data (Tubs), which is most easily done by installing the Donkey library and use its utility classes for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List installed python libraries. tensorflow-gpu (GPU-optimized) should be installed.\n",
    "!conda list | grep -i tensorflow\n",
    "!conda list | grep -i donkey\n",
    "\n",
    "# Clone donkey git\n",
    "%cd ~/SageMaker\n",
    "!rm -rf ~/SageMaker/donkey\n",
    "!git clone https://github.com/wroscoe/donkey\n",
    "\n",
    "# Donkey has dependencies to tensorflow (non-GPU) and keras, none of which we are interested in.\n",
    "# Remove Keras and replace tensorflow with tensorflow-gpu\n",
    "!sed -i -e '/keras==2.0.8/d' donkey/setup.py\n",
    "!sed -i -e 's/tensorflow>=1.1/tensorflow-gpu>=1.4/g' donkey/setup.py\n",
    "\n",
    "# Install Donkey\n",
    "!pip uninstall donkeycar --yes\n",
    "!pip install ./donkey\n",
    "!pip show donkeycar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some globals for now\n",
    "BATCH_SIZE = 128\n",
    "TEST_SPLIT = 0.8\n",
    "EPOCHS = 5               # <---- NOTE! Using only 5 epochs for now, to speed up test-training...\n",
    "\n",
    "import os\n",
    "from donkeycar.parts.datastore import TubGroup\n",
    "from donkeycar.utils import linear_bin\n",
    "\n",
    "def train(tub_names, model_name):\n",
    "    '''\n",
    "    Convenience method for training using MyEstimator\n",
    "    \n",
    "    Requires the TubGroup class from Donkey to read Tub data.\n",
    "    '''\n",
    "    x_keys = ['cam/image_array']\n",
    "    y_keys = ['user/angle', 'user/throttle']\n",
    "\n",
    "    def rt(record):\n",
    "        record['user/angle'] = linear_bin(record['user/angle'])\n",
    "        return record\n",
    "\n",
    "    tubgroup = TubGroup(tub_names)\n",
    "    train_gen, val_gen = tubgroup.get_train_val_gen(x_keys,\n",
    "                                                    y_keys,\n",
    "                                                    record_transform=rt,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    train_frac=TEST_SPLIT)\n",
    "\n",
    "    model_path = os.path.expanduser(model_name)\n",
    "\n",
    "    total_records = len(tubgroup.df)\n",
    "    total_train = int(total_records * TEST_SPLIT)\n",
    "    total_val = total_records - total_train\n",
    "    print('train: %d, validation: %d' % (total_train, total_val))\n",
    "    steps_per_epoch = total_train // BATCH_SIZE\n",
    "    print('steps_per_epoch', steps_per_epoch)\n",
    "\n",
    "    kl = MyEstimator()\n",
    "    kl.train(train_gen,\n",
    "             val_gen,\n",
    "             saved_model_path=model_path,\n",
    "             steps=steps_per_epoch,\n",
    "             train_split=TEST_SPLIT,\n",
    "             epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Tub\n",
    "sample_data_location = 's3://jayway-robocar-raw-data/samples'\n",
    "!aws s3 cp {sample_data_location}/ore.zip /tmp/ore.zip\n",
    "!mkdir -pv ~/SageMaker/data\n",
    "!unzip /tmp/ore.zip -d ~/SageMaker/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "!mkdir -pv ~/SageMaker/models\n",
    "tub = '~/SageMaker/data/tub_8_18-02-09'\n",
    "model = '~/SageMaker/models/my-cloud-model'\n",
    "\n",
    "train(tub, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new model\n",
    "\n",
    "Test the new model using either the car or simulator (see [donkey-train.ipynb](./donkey-train.ipynb#test-the-new-model))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "[Visualization](./donkey-board.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Donkey Convolutional Neural Network\n",
    "\n",
    "The goal of this chapter is to get familiar with the neural network(s) used in the [Donkey](https://github.com/wroscoe/donkey) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Donkey library\n",
    "\n",
    "The [Donkey library](https://github.com/wroscoe/donkey) has several components.\n",
    "\n",
    "It is first and foremost a python library installed where your other python libraries are (e.g. system python or virtualenv). After installation, you can `import` it as any normal python library:\n",
    "\n",
    "```python\n",
    "import donkeycar as dk\n",
    "```\n",
    "\n",
    "It also has a CLI with tools mainly used to aid training (see [donkey-tools.ipynb](./donkey-tools.ipynb)):\n",
    "\n",
    "```bash\n",
    "donkey --help\n",
    "```\n",
    "\n",
    "A `Vehicle` application, installed to the `~/d2` directory by default. This is where you'll find the `manage.py` script, which is used for both **driving** and **training**.\n",
    "\n",
    "```bash\n",
    "~/d2/manage.py --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Donkey library\n",
    "\n",
    "If you already installed the [Donkey](https://github.com/wroscoe/donkey) library, you can [skip](#Train) this step.\n",
    "\n",
    "Otherwise, go ahead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're in SageMaker root\n",
    "%cd ~/SageMaker\n",
    "\n",
    "# Remove any old versions of the library\n",
    "!rm -rf ~/SageMaker/donkey\n",
    "\n",
    "# Clone the Donkey library git\n",
    "!git clone https://github.com/wroscoe/donkey.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Donkey dependencies\n",
    "\n",
    "# Keras is pinned to version 2.0.8 in the Donkey requirements. Change this to allow a newer version\n",
    "!sed -i -e 's/keras==2.0.8/keras>=2.1.2/g' ~/SageMaker/donkey/setup.py\n",
    "!sed -i -e 's/tensorflow>=1.1/tensorflow-gpu>=1.4/g' ~/SageMaker/donkey/setup.py\n",
    "\n",
    "# Install\n",
    "!pip uninstall --yes donkeycar\n",
    "!pip install ~/SageMaker/donkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Keras network\n",
    "\n",
    "First, take a few minutes to look through the `keras.py` file (it's a [Donkey](https://github.com/wroscoe/donkey) library *part*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming donkey library is installed in ./donkey\n",
    "%pycat ~/SageMaker/donkey/donkeycar/parts/keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default algorithm used is defined in `donkey/donkeycar/templates/donkey2.py`:\n",
    "\n",
    "```python\n",
    "def drive(cfg, model_path=None, use_joystick=False):\n",
    "    ...\n",
    "    kl = KerasCategorical()\n",
    "    if model_path:\n",
    "        kl.load(model_path)\n",
    "    ...\n",
    "```\n",
    "\n",
    "`KerasCategorical`, which is created in `default_categorical()` method. Before looking closer at the code, let's see if we can visualize it using our pre-trained model (see [Donkey library training](./donkey-train.ipynb) chapter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model (if it's not already there)\n",
    "# Replace bucket if you used some other bucket than SageMaker default\n",
    "import sagemaker\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "!aws s3 cp s3://{bucket}/models/my-first-model ~/SageMaker/models/my-first-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install some additional python libraries required by the Keras visualization lib\n",
    "!conda install --yes pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the algorithm\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('./my-first-model')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! A nice neural network with\n",
    "- 1 input layer\n",
    "- 5 convolutional layers\n",
    "- 1 flatten layer\n",
    "- 2 dense layers\n",
    "- 2 dropout layers\n",
    "\n",
    "The presence of a convolutional layer makes this neural network a [convolutional network](https://en.wikipedia.org/wiki/Convolutional_neural_network). This makes sense, since CNNs are particulary good with images.\n",
    "\n",
    "So, what do the different layers actually do? Let's have a look at them, one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick readup\n",
    "\n",
    "This excellent cheatsheet is handy to read through and use as a reference before continuing:\n",
    "- http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html\n",
    "\n",
    "And two links about CNNs (read, or be confused later):\n",
    "- https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "- https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input layer\n",
    "\n",
    "The *Input layer* holds the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "# Input layer\n",
    "#\n",
    "# 120 x 160 x 3 image size (RGB)\n",
    "#\n",
    "img_in = Input(shape=(120, 160, 3), name='img_in')\n",
    "\n",
    "# Rename it to x\n",
    "x = img_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the shape of the data as a 3 channel (RGB) 120px x 160px image.\n",
    "\n",
    "If you read through the [links](#quick-readup), you'll have a rought idea of what a channel is. In CNNs,  layers operate on 3D chunks of data. The first two dimensions are the height and width of the image, and the third dimension is a number of such 2D data stacked over each other (3 in RGB images). This stack is called channels.\n",
    "\n",
    "Why is this important? In the input layer, a channel is easily understood as RGB data (one channel per color). However, a [convolutional layer](#first-convolutional-layer) can have many more channels, which we'll get to later.\n",
    "\n",
    "Formally, x can be viewed as a $H \\times W \\times C$ vector, where `H,W` are the dimensions of the image and `C` is the number of channels of the given image volume.\n",
    "\n",
    "API documentation:\n",
    "- https://keras.io/layers/core/#input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First convolutional layer\n",
    "\n",
    "Before continuing, make sure you've read through and have a rough idea of the following concepts:\n",
    "- NN concepts - http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html\n",
    "- Relu - http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu\n",
    "- Convolution operator - https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html#convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D\n",
    "\n",
    "# First hidden layer\n",
    "#\n",
    "# 24 features      - Results in a 24 channel feature map(s). This means the first layer can detect 24 different low level features in the image.\n",
    "# 5x5 pixel kernel - Width and height of the kernel. Will automatically have the same channel depth as the input (5x5x3)\n",
    "# 2w x 2h stride   - Strides of the convolution along the width and height\n",
    "# relu activation  - Use the 'relu' activation function\n",
    "#\n",
    "x = Convolution2D(24, (5,5), strides=(2,2), activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hidden layer is a 2D convolution layer with the described hyperparameters (see above) and a `relu` activation function.\n",
    "\n",
    "> What is the reasoning behind this design? Why are the hyperparameters given these values?\n",
    "\n",
    "This is a tricky question to answer, but important for later tweaking of the network. The sad news are that they often required lots of experience and theoretical background to master. But hey, let's give it a shot!\n",
    "\n",
    "They ar all a tradeoff between performance and accuracy, but there are some general rules to follow. For example, the number of *features* are usually lower in the first convolutional layers, because the input size is still large, resulting in large *feature maps*. Later layers have smaller (but deeper) input size (because of the convolution in the previous layers), and thus can afford to have more features. Similar reasoning can be done for the other hyperparameters. The following link has a more in depth discussion around CNN hyperparameters:\n",
    "- http://deeplearning.net/tutorial/lenet.html#tips-and-tricks\n",
    "\n",
    "API documentation:\n",
    "- https://keras.io/layers/convolutional/#conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second hidden layer\n",
    "#\n",
    "# 32 features      - Results in a 32 channel feature map(s)\n",
    "#\n",
    "x = Convolution2D(32, (5,5), strides=(2,2), activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much changed here. We increase the number of features as discussed earlier. This allows the network to pick up more features based on the feature maps from the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third hidden layer\n",
    "#\n",
    "# 64 features      - Results in a 64 channel feature map(s)\n",
    "#\n",
    "x = Convolution2D(64, (5,5), strides=(2,2), activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increase the number of features yet again. The input to the next layer will now be 64 channels deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth hidden layer\n",
    "#\n",
    "# 3x3 pixel kernel   - Width and height of the kernel. Will automatically have the same channel depth as the input (3x3x64)\n",
    "x = Convolution2D(64, (3,3), strides=(2,2), activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decrease the kernel size to 3x3. By using a relatively large kernel size in the 3 previous layers, we have shrunk the image size by quite a bit (each convolution will shrink the image/feature map (if not padded), and add depth instead). By decreasing the kernel size, we make sure that image doesn't shrink too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fifth convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth hidden layer\n",
    "#\n",
    "# 1w x 1h stride   -\n",
    "x = Convolution2D(64, (3,3), strides=(1,1), activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last convolution layer, we change the stride length. This will result in larger feature maps (at the cost of performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten \n",
    "\n",
    "# Flatten layer\n",
    "#\n",
    "# Flatten to 1 dimension\n",
    "#\n",
    "x = Flatten(name='flattened')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flatten layer does what it claims to do. It flattens the convoluted input from 64 channels to 1, by putting the channels after each other. This is needed for the next layer, a fully-connected MLP (Dense).\n",
    "\n",
    "API documentation:\n",
    "- https://keras.io/layers/core/#flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# First Dense layer\n",
    "#\n",
    "# 100 units       - Use 100 neurons in the layer\n",
    "# relu activation - Use 'relu' activation\n",
    "#\n",
    "x = Dense(100, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dense layer is another name for a fully-connected layer like in a [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron). It is very common for upper-layers in a CNNs to have fully-connected layers (actually, the purpose of conv layers is to extract important features from the image before downsampling enough to be handled by a MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# First dropout\n",
    "#\n",
    "# 0.1 - Randomly drop out 10% of the neurons to prevent overfitting.\n",
    "#\n",
    "x = Dropout(.1)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the [links](), dropout is used to prevent overfitting by forcing redundancy in the neural network (which means it cannot rely on a particular neuron because it might be dropped)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Dense layer\n",
    "#\n",
    "# 50 units        - Use 50 neurons in the layer\n",
    "#\n",
    "x = Dense(50, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gradually decrease the size of the layers from the original 100 to 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second dropout\n",
    "#\n",
    "# Not much to say here...\n",
    "#\n",
    "x = Dropout(.1)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs\n",
    "#\n",
    "# Angle\n",
    "# Dense              - Fully-connected output layer\n",
    "# 15 units           - Use a 15 neuron output.\n",
    "# softmax activation - Use 'softmax' activation\n",
    "#\n",
    "# Throttle\n",
    "# Dense           - Fully-connected output layer\n",
    "# 1 unit          - Use a 1 neuron output\n",
    "# relu activation - Use 'relu' activation\n",
    "angle_out = Dense(15, activation='softmax', name='angle_out')(x)\n",
    "throttle_out = Dense(1, activation='relu', name='throttle_out')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layers are fully-connected with different units and activations.\n",
    "\n",
    "Angle uses a 15 neuron output with a softmax activation. [Softmax](https://en.wikipedia.org/wiki/Softmax_function) is a common way of creating a probability distribution over K different possible outcomes (in this case, `K=15`).\n",
    "\n",
    "Throttle uses a 1 neuron output with a relu activation. [Relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) will result in the throttle having only positive values (it can only go forward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "\n",
    "Finally, it's time to create the model and define the loss functions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# Create model\n",
    "#\n",
    "# Optimizer\n",
    "# ---------\n",
    "# adam\n",
    "#\n",
    "# Angle\n",
    "# -----\n",
    "# categorical cross entropy loss function\n",
    "# 0.9 loss weight\n",
    "#\n",
    "# Throttle\n",
    "# --------\n",
    "# mean_absolute_error loss function\n",
    "# 0.001 loss weight\n",
    "# \n",
    "model = Model(inputs=[img_in], outputs=[angle_out, throttle_out])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'angle_out': 'categorical_crossentropy', 'throttle_out': 'mean_absolute_error'},\n",
    "    loss_weights={'angle_out': 0.9, 'throttle_out': .001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model requires the following information:\n",
    "- input (`img_in`) and output tensors (`angle_out`, `throttle_out`)\n",
    "- optimizer ([`adam`](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)). The optimizer is the algorithm used when minimizing the loss function during training. Performs well and efficient.\n",
    "- loss function(s), one for each output\n",
    "  - `angle_out` : [`categorial_crossentropy`](https://en.wikipedia.org/wiki/Cross_entropy) - Suitable for categorization.\n",
    "  - `throttle_out` : [`mean_absolute_error`](https://en.wikipedia.org/wiki/Mean_absolute_error) - More general loss function\n",
    "- initial loss weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next\n",
    "\n",
    "[Donkey data](./donkey-data.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
